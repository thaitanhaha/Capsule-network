{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1504266,"sourceType":"datasetVersion","datasetId":885385},{"sourceId":1960165,"sourceType":"datasetVersion","datasetId":1170109},{"sourceId":227433307,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Subset\nfrom torch.utils.data import DataLoader\nfrom collections import Counter\nfrom torch.optim import Adam\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:43.723708Z","iopub.execute_input":"2025-03-14T10:29:43.724059Z","iopub.status.idle":"2025-03-14T10:29:43.729323Z","shell.execute_reply.started":"2025-03-14T10:29:43.724030Z","shell.execute_reply":"2025-03-14T10:29:43.728306Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"human_faces_dir = '/kaggle/input/human-faces/Humans'\nflowers_dir = '/kaggle/input/face-dataset/human-swap/'\n\nhuman_faces_files = len(os.listdir(human_faces_dir))\nflowers_files = len(os.listdir(flowers_dir))\n\nprint(f\"Number of files in human_faces_dir: {human_faces_files}\")\nprint(f\"Number of files in flowers_dir: {flowers_files}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:43.730670Z","iopub.execute_input":"2025-03-14T10:29:43.730992Z","iopub.status.idle":"2025-03-14T10:29:43.748565Z","shell.execute_reply.started":"2025-03-14T10:29:43.730962Z","shell.execute_reply":"2025-03-14T10:29:43.747721Z"}},"outputs":[{"name":"stdout","text":"Number of files in human_faces_dir: 7219\nNumber of files in flowers_dir: 6676\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"my_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\nclass CustomDataset(Dataset):\n    def __init__(self, human_faces_dir, flowers_dir, transform=None):\n        self.human_faces_dir = human_faces_dir\n        self.flowers_dir = flowers_dir\n        \n        human_faces_images = [os.path.join(human_faces_dir, fname) for fname in os.listdir(human_faces_dir)]\n        self.human_faces_images = random.sample(human_faces_images, 300)\n        flowers_images = [os.path.join(flowers_dir, fname) for fname in os.listdir(flowers_dir)]\n        self.flowers_images = random.sample(flowers_images, 200)\n        \n        self.all_images = self.human_faces_images + self.flowers_images\n        self.labels = [1] * len(self.human_faces_images) + [0] * len(self.flowers_images)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.all_images)\n\n    def __getitem__(self, idx):\n        img_path = self.all_images[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        label = self.labels[idx]\n        \n        if self.transform: image = self.transform(image)\n        return image, label\n\ndataset = CustomDataset(human_faces_dir, flowers_dir, transform=my_transform)\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:43.750545Z","iopub.execute_input":"2025-03-14T10:29:43.750759Z","iopub.status.idle":"2025-03-14T10:29:43.781270Z","shell.execute_reply.started":"2025-03-14T10:29:43.750740Z","shell.execute_reply":"2025-03-14T10:29:43.780617Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"for images, labels in dataloader:\n    print(images.shape)\n    print(labels)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:43.782089Z","iopub.execute_input":"2025-03-14T10:29:43.782297Z","iopub.status.idle":"2025-03-14T10:29:46.572412Z","shell.execute_reply.started":"2025-03-14T10:29:43.782269Z","shell.execute_reply":"2025-03-14T10:29:46.571469Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 128, 128])\ntensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 0])\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, test_dir, transform=None):\n        self.test_dir = test_dir\n        test_images = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith(('.jpg', '.png', '.jpeg'))]\n        test_images = random.sample(test_images, 128)\n        self.all_images = test_images\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.all_images)\n\n    def __getitem__(self, idx):\n        img_path = self.all_images[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform: image = self.transform(image)\n        return image\n\ntest_dir = \"/kaggle/input/face-dataset/human-swap/\"\ntest_dataset = TestDataset(test_dir, transform=my_transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:46.573360Z","iopub.execute_input":"2025-03-14T10:29:46.573661Z","iopub.status.idle":"2025-03-14T10:29:46.588735Z","shell.execute_reply.started":"2025-03-14T10:29:46.573636Z","shell.execute_reply":"2025-03-14T10:29:46.587973Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"for images in test_dataloader:\n    print(images.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:46.589699Z","iopub.execute_input":"2025-03-14T10:29:46.590076Z","iopub.status.idle":"2025-03-14T10:29:51.048437Z","shell.execute_reply.started":"2025-03-14T10:29:46.590006Z","shell.execute_reply":"2025-03-14T10:29:51.047523Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 128, 128])\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Capsule","metadata":{}},{"cell_type":"code","source":"def debug_print(debug, message, tensor=None):\n    if debug:\n        if tensor is not None:\n            print(f\"{message}: {tensor.shape}\")\n        else:\n            print(message)\n\ndef squash(input_tensor, epsilon=1e-7):\n    squared_norm = (input_tensor ** 2 + epsilon).sum(-1, keepdim=True)\n    output_tensor = (squared_norm / (1. + squared_norm)) *  (input_tensor / torch.sqrt(squared_norm))\n    return output_tensor\n\nclass ConvLayer(nn.Module):\n    def __init__(self, in_channels=3, out_channels=256, kernel_size=9, debug=False):\n        super(ConvLayer, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1)\n        self.debug = debug\n\n    def forward(self, x):\n        x = self.conv(x)\n        debug_print(self.debug, \"x after conv\", x)\n        x = F.relu(x)\n        debug_print(self.debug, \"x after ReLU\", x)\n        return x\n\n\nclass PrimaryCaps(nn.Module):\n    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9, debug=False):\n        super(PrimaryCaps, self).__init__()\n        self.capsules = nn.ModuleList()\n        for _ in range(num_capsules):\n            capsule = nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=9, stride=2, padding=0),\n                nn.Conv2d(in_channels=128, out_channels=64, kernel_size=9, stride=2, padding=0),\n                nn.Conv2d(in_channels=64, out_channels=out_channels, kernel_size=9, stride=3, padding=0)\n            )\n            self.capsules.append(capsule)\n        self.debug = debug\n    \n    def forward(self, x):\n        stacked_capsules = [capsule(x) for capsule in self.capsules]\n        if self.debug: \n            print(\"capsule_out:\")\n            for capsule_out in stacked_capsules:\n                print(\"\\t\", capsule_out.shape)\n        stacked_capsules = torch.stack(stacked_capsules, dim=1)\n        debug_print(self.debug, \"stacked_capsules\", stacked_capsules)\n        flattened_capsules = stacked_capsules.view(x.size(0), 32 * 6 * 6, -1)\n        debug_print(self.debug, \"flattened_capsules\", flattened_capsules)\n        squashed_output = squash(flattened_capsules)\n        debug_print(self.debug, \"squashed_output\", squashed_output)\n        return squashed_output\n\n\nclass DigitCaps(nn.Module):\n    def __init__(self, num_capsules=2, num_routes=32 * 6 * 6, in_channels=8, out_channels=16, debug=False):\n        super(DigitCaps, self).__init__()\n        self.in_channels = in_channels\n        self.num_routes = num_routes\n        self.num_capsules = num_capsules\n        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n        self.debug = debug\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n        debug_print(self.debug, \"x after stacking\", x)\n\n        W = torch.cat([self.W] * batch_size, dim=0)\n        debug_print(self.debug, \"W\", W)\n        \n        u_hat = torch.matmul(W, x)\n        debug_print(self.debug, \"u_hat\", u_hat)\n\n        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n        b_ij = b_ij.to(device)\n        debug_print(self.debug, \"b_ij\", b_ij)\n\n        num_iter = 3\n        for i in range(num_iter):\n            if self.debug: print()\n            c_ij = F.softmax(b_ij, dim=1)\n            debug_print(self.debug, \"c_ij\", c_ij)\n            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n            debug_print(self.debug, \"c_ij after repeat\", c_ij)\n\n            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n            debug_print(self.debug, \"s_j\", s_j)\n\n            v_j = squash(s_j)\n            debug_print(self.debug, \"v_j\", v_j)\n            \n            a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n            debug_print(self.debug, \"a_ij\", a_ij)\n            b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n            debug_print(self.debug, \"b_ij updated\", b_ij)\n\n        return v_j.squeeze(1)\n\nclass Decoder(nn.Module):\n    def __init__(self, debug=False):\n        super(Decoder, self).__init__()\n        self.debug = debug\n        \n    def forward(self, x, data):\n        classes = torch.sqrt((x ** 2).sum(2))\n        debug_print(self.debug, \"classes before softmax\", classes)\n        classes = F.softmax(classes, dim=0)\n        debug_print(self.debug, \"classes after softmax\", classes)\n        \n        _, max_length_indices = classes.max(dim=1)\n        debug_print(self.debug, \"max_length_indices\", max_length_indices)\n        masked = Variable(torch.sparse.torch.eye(2))\n        debug_print(self.debug, \"masked\", masked)\n        masked = masked.to(device)\n        masked = masked.index_select(dim=0, index=Variable(max_length_indices.squeeze(1).data))\n        debug_print(self.debug, \"masked after index_select\", masked)\n        \n        return masked","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:51.049950Z","iopub.execute_input":"2025-03-14T10:29:51.050202Z","iopub.status.idle":"2025-03-14T10:29:51.066654Z","shell.execute_reply.started":"2025-03-14T10:29:51.050176Z","shell.execute_reply":"2025-03-14T10:29:51.065866Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class CapsuleNet(nn.Module):\n    def __init__(self, debug=False):\n        super(CapsuleNet, self).__init__()\n        self.conv_layer = ConvLayer(debug=debug)\n        self.primary_capsules = PrimaryCaps(debug=debug)\n        self.digit_capsules = DigitCaps(debug=debug)\n        self.decoder = Decoder(debug=debug)\n        self.mse_loss = nn.MSELoss()\n        self.debug = debug\n        \n    def forward(self, data):\n        debug_print(self.debug, f\"Input data\", data)\n        debug_print(self.debug, \"\\nCONV\")\n        output = self.conv_layer(data)\n        debug_print(self.debug, \"\\nPRIMARY\")\n        output = self.primary_capsules(output)\n        debug_print(self.debug, \"\\nDIGIT\")\n        output = self.digit_capsules(output)\n        debug_print(self.debug, \"\\nDECODER\")\n        masked = self.decoder(output, data)\n        debug_print(self.debug, \"\\nOUTPUT\", output)\n        return output, masked\n\n    def loss(self, x, target):\n        return self.margin_loss(x, target)\n    \n    def margin_loss(self, x, labels):\n        batch_size = x.size(0)\n        v_k = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n        left = F.relu(0.9 - v_k).view(batch_size, -1)\n        right = F.relu(v_k - 0.1).view(batch_size, -1)\n        loss = labels * left + 0.5 * (1.0 - labels) * right\n        loss = loss.sum(dim=1).mean()\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:51.067644Z","iopub.execute_input":"2025-03-14T10:29:51.067876Z","iopub.status.idle":"2025-03-14T10:29:51.086037Z","shell.execute_reply.started":"2025-03-14T10:29:51.067847Z","shell.execute_reply":"2025-03-14T10:29:51.085309Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"capsule_net = CapsuleNet(debug=True).to(device) \ndata, target = next(iter(dataloader))\ntarget = torch.sparse.torch.eye(2).index_select(dim=0, index=target)\ndata, target = Variable(data), Variable(target)\ndata, target = data.to(device), target.to(device)\noutput, masked = capsule_net(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:51.086771Z","iopub.execute_input":"2025-03-14T10:29:51.087029Z","iopub.status.idle":"2025-03-14T10:29:57.219390Z","shell.execute_reply.started":"2025-03-14T10:29:51.086997Z","shell.execute_reply":"2025-03-14T10:29:57.218594Z"}},"outputs":[{"name":"stdout","text":"Input data: torch.Size([128, 3, 128, 128])\n\nCONV\nx after conv: torch.Size([128, 256, 120, 120])\nx after ReLU: torch.Size([128, 256, 120, 120])\n\nPRIMARY\ncapsule_out:\n\t torch.Size([128, 32, 6, 6])\n\t torch.Size([128, 32, 6, 6])\n\t torch.Size([128, 32, 6, 6])\n\t torch.Size([128, 32, 6, 6])\n\t torch.Size([128, 32, 6, 6])\n\t torch.Size([128, 32, 6, 6])\n\t torch.Size([128, 32, 6, 6])\n\t torch.Size([128, 32, 6, 6])\nstacked_capsules: torch.Size([128, 8, 32, 6, 6])\nflattened_capsules: torch.Size([128, 1152, 8])\nsquashed_output: torch.Size([128, 1152, 8])\n\nDIGIT\nx after stacking: torch.Size([128, 1152, 2, 8, 1])\nW: torch.Size([128, 1152, 2, 16, 8])\nu_hat: torch.Size([128, 1152, 2, 16, 1])\nb_ij: torch.Size([1, 1152, 2, 1])\n\nc_ij: torch.Size([1, 1152, 2, 1])\nc_ij after repeat: torch.Size([128, 1152, 2, 1, 1])\ns_j: torch.Size([128, 1, 2, 16, 1])\nv_j: torch.Size([128, 1, 2, 16, 1])\na_ij: torch.Size([128, 1152, 2, 1, 1])\nb_ij updated: torch.Size([1, 1152, 2, 1])\n\nc_ij: torch.Size([1, 1152, 2, 1])\nc_ij after repeat: torch.Size([128, 1152, 2, 1, 1])\ns_j: torch.Size([128, 1, 2, 16, 1])\nv_j: torch.Size([128, 1, 2, 16, 1])\na_ij: torch.Size([128, 1152, 2, 1, 1])\nb_ij updated: torch.Size([1, 1152, 2, 1])\n\nc_ij: torch.Size([1, 1152, 2, 1])\nc_ij after repeat: torch.Size([128, 1152, 2, 1, 1])\ns_j: torch.Size([128, 1, 2, 16, 1])\nv_j: torch.Size([128, 1, 2, 16, 1])\na_ij: torch.Size([128, 1152, 2, 1, 1])\nb_ij updated: torch.Size([1, 1152, 2, 1])\n\nDECODER\nclasses before softmax: torch.Size([128, 2, 1])\nclasses after softmax: torch.Size([128, 2, 1])\nmax_length_indices: torch.Size([128, 1])\nmasked: torch.Size([2, 2])\nmasked after index_select: torch.Size([128, 2])\n\nOUTPUT: torch.Size([128, 2, 16, 1])\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"capsule_net = CapsuleNet().to(device) \noptimizer = Adam(capsule_net.parameters())\n\nn_epochs = 20\n\nfor epoch in range(n_epochs):\n    capsule_net.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch_id, (data, target) in enumerate(tqdm(dataloader)):\n        target = torch.sparse.torch.eye(2).index_select(dim=0, index=target)\n        data, target = Variable(data), Variable(target)\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output, masked = capsule_net(data)\n        loss = capsule_net.loss(output, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n        preds = np.argmax(masked.data.cpu().numpy(), axis=1)\n        targets = np.argmax(target.data.cpu().numpy(), axis=1)\n        correct_train += np.sum(preds == targets)\n        total_train += len(targets)\n    \n    train_accuracy = correct_train / total_train\n    avg_train_loss = train_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_train_loss:.4f} - Accuracy: {train_accuracy:.4f}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:29:57.220249Z","iopub.execute_input":"2025-03-14T10:29:57.220482Z","iopub.status.idle":"2025-03-14T10:52:10.454057Z","shell.execute_reply.started":"2025-03-14T10:29:57.220460Z","shell.execute_reply":"2025-03-14T10:52:10.453180Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [01:10<00:00, 17.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Loss: 0.8335 - Accuracy: 0.5120\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:07<00:00, 16.76s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Loss: 0.7427 - Accuracy: 0.5540\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Loss: 0.6956 - Accuracy: 0.5280\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Loss: 0.6626 - Accuracy: 0.5260\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Loss: 0.6410 - Accuracy: 0.5560\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 - Loss: 0.6244 - Accuracy: 0.5420\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 - Loss: 0.6123 - Accuracy: 0.5400\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 - Loss: 0.6016 - Accuracy: 0.5520\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 - Loss: 0.5933 - Accuracy: 0.5620\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 - Loss: 0.5844 - Accuracy: 0.5600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 - Loss: 0.5753 - Accuracy: 0.5840\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 - Loss: 0.5675 - Accuracy: 0.5820\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 - Loss: 0.5571 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 - Loss: 0.5501 - Accuracy: 0.6360\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 - Loss: 0.5416 - Accuracy: 0.6280\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 - Loss: 0.5280 - Accuracy: 0.6380\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 - Loss: 0.5115 - Accuracy: 0.6580\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 - Loss: 0.4895 - Accuracy: 0.6500\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:06<00:00, 16.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 - Loss: 0.4659 - Accuracy: 0.6080\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [01:05<00:00, 16.49s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 - Loss: 0.4521 - Accuracy: 0.6240\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"capsule_net.eval()\ntest_loss = 0\ncorrect_test = 0\ntotal_test = 0\n\nwith torch.no_grad():\n    for data in test_dataloader:\n        target = torch.sparse.torch.eye(2).index_select(dim=0, index=torch.tensor([0] * 128))\n        data, target = Variable(data), Variable(target)\n        data, target = data.to(device), target.to(device)\n    \n        output, masked = capsule_net(data)\n        loss = capsule_net.loss(output, target)\n    \n        test_loss += loss.item()\n    \n        preds = np.argmax(masked.data.cpu().numpy(), axis=1)\n        targets = np.argmax(target.data.cpu().numpy(), axis=1)\n        print(preds)\n        print(targets)\n        correct_test += np.sum(preds == targets)\n        total_test += len(targets)\n    \n    test_accuracy = correct_test / total_test\n    avg_test_loss = test_loss / len(test_dataloader)\n    print(f\"Loss: {avg_test_loss:.4f} - Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:52:58.426398Z","iopub.execute_input":"2025-03-14T10:52:58.426693Z","iopub.status.idle":"2025-03-14T10:53:03.441558Z","shell.execute_reply.started":"2025-03-14T10:52:58.426670Z","shell.execute_reply":"2025-03-14T10:53:03.440735Z"}},"outputs":[{"name":"stdout","text":"[1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1\n 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0\n 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0]\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nLoss: 0.8182 - Accuracy: 0.4609\n","output_type":"stream"}],"execution_count":29}]}