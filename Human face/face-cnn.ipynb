{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1504266,"sourceType":"datasetVersion","datasetId":885385},{"sourceId":1960165,"sourceType":"datasetVersion","datasetId":1170109},{"sourceId":227433307,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Subset\nfrom torch.utils.data import DataLoader\nfrom collections import Counter\nfrom torch.optim import Adam\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:51:01.695933Z","iopub.execute_input":"2025-03-13T20:51:01.696224Z","iopub.status.idle":"2025-03-13T20:51:07.885575Z","shell.execute_reply.started":"2025-03-13T20:51:01.696201Z","shell.execute_reply":"2025-03-13T20:51:07.884949Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"human_faces_dir = '/kaggle/input/human-faces/Humans'\nflowers_dir = '/kaggle/input/flowers-dataset/test'\n\nhuman_faces_files = len(os.listdir(human_faces_dir))\nflowers_files = len(os.listdir(flowers_dir))\n\nprint(f\"Number of files in human_faces_dir: {human_faces_files}\")\nprint(f\"Number of files in flowers_dir: {flowers_files}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:51:07.886597Z","iopub.execute_input":"2025-03-13T20:51:07.887028Z","iopub.status.idle":"2025-03-13T20:51:07.986852Z","shell.execute_reply.started":"2025-03-13T20:51:07.887004Z","shell.execute_reply":"2025-03-13T20:51:07.986066Z"}},"outputs":[{"name":"stdout","text":"Number of files in human_faces_dir: 7219\nNumber of files in flowers_dir: 924\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"my_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\nclass CustomDataset(Dataset):\n    def __init__(self, human_faces_dir, flowers_dir, transform=None):\n        self.human_faces_dir = human_faces_dir\n        self.flowers_dir = flowers_dir\n        \n        human_faces_images = [os.path.join(human_faces_dir, fname) for fname in os.listdir(human_faces_dir)]\n        self.human_faces_images = random.sample(human_faces_images, 300)\n        flowers_images = [os.path.join(flowers_dir, fname) for fname in os.listdir(flowers_dir)]\n        self.flowers_images = random.sample(flowers_images, 200)\n        \n        self.all_images = self.human_faces_images + self.flowers_images\n        self.labels = [1] * len(self.human_faces_images) + [0] * len(self.flowers_images)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.all_images)\n\n    def __getitem__(self, idx):\n        img_path = self.all_images[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        label = self.labels[idx]\n        \n        if self.transform: image = self.transform(image)\n        return image, label\n\ndataset = CustomDataset(human_faces_dir, flowers_dir, transform=my_transform)\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:51:07.988485Z","iopub.execute_input":"2025-03-13T20:51:07.988724Z","iopub.status.idle":"2025-03-13T20:51:08.082950Z","shell.execute_reply.started":"2025-03-13T20:51:07.988670Z","shell.execute_reply":"2025-03-13T20:51:08.081965Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"for images, labels in dataloader:\n    print(images.shape)\n    print(labels)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:51:08.083975Z","iopub.execute_input":"2025-03-13T20:51:08.084264Z","iopub.status.idle":"2025-03-13T20:51:13.284943Z","shell.execute_reply.started":"2025-03-13T20:51:08.084242Z","shell.execute_reply":"2025-03-13T20:51:13.284088Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 128, 128])\ntensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n        1, 0, 1, 0, 0, 1, 0, 0])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, test_dir, transform=None):\n        self.test_dir = test_dir\n        test_images = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith(('.jpg', '.png', '.jpeg'))]\n        test_images = random.sample(test_images, 128)\n        self.all_images = test_images\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.all_images)\n\n    def __getitem__(self, idx):\n        img_path = self.all_images[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform: image = self.transform(image)\n        return image\n\ntest_dir = \"/kaggle/input/face-dataset/human-swap/\"\ntest_dataset = TestDataset(test_dir, transform=my_transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:51:13.285761Z","iopub.execute_input":"2025-03-13T20:51:13.286117Z","iopub.status.idle":"2025-03-13T20:51:13.377331Z","shell.execute_reply.started":"2025-03-13T20:51:13.286093Z","shell.execute_reply":"2025-03-13T20:51:13.376347Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"for images in test_dataloader:\n    print(images.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:51:13.378836Z","iopub.execute_input":"2025-03-13T20:51:13.379171Z","iopub.status.idle":"2025-03-13T20:51:16.648428Z","shell.execute_reply.started":"2025-03-13T20:51:13.379129Z","shell.execute_reply":"2025-03-13T20:51:16.647658Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 128, 128])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Capsule","metadata":{}},{"cell_type":"code","source":"def debug_print(debug, message, tensor=None):\n    if debug:\n        if tensor is not None:\n            print(f\"{message}: {tensor.shape}\")\n        else:\n            print(message)\n\ndef squash(input_tensor, epsilon=1e-7):\n    squared_norm = (input_tensor ** 2 + epsilon).sum(-1, keepdim=True)\n    output_tensor = (squared_norm / (1. + squared_norm)) *  (input_tensor / torch.sqrt(squared_norm))\n    return output_tensor\n\nclass ConvLayer(nn.Module):\n    def __init__(self, in_channels=1, out_channels=256, kernel_size=9, debug=False):\n        super(ConvLayer, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1)\n        self.debug = debug\n\n    def forward(self, x):\n        x = self.conv(x)\n        debug_print(self.debug, \"x after conv\", x)\n        x = F.relu(x)\n        debug_print(self.debug, \"x after ReLU\", x)\n        return x\n\n\nclass PrimaryCaps(nn.Module):\n    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9, debug=False):\n        super(PrimaryCaps, self).__init__()\n        self.capsules = nn.ModuleList()\n        for _ in range(num_capsules):\n            capsule = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0)\n            self.capsules.append(capsule)\n        self.debug = debug\n    \n    def forward(self, x):\n        stacked_capsules = [capsule(x) for capsule in self.capsules]\n        if self.debug: \n            print(\"capsule_out:\")\n            for capsule_out in stacked_capsules:\n                print(\"\\t\", capsule_out.shape)\n        stacked_capsules = torch.stack(stacked_capsules, dim=1)\n        debug_print(self.debug, \"stacked_capsules\", stacked_capsules)\n        flattened_capsules = stacked_capsules.view(x.size(0), 32 * 6 * 6, -1)\n        debug_print(self.debug, \"flattened_capsules\", flattened_capsules)\n        squashed_output = squash(flattened_capsules)\n        debug_print(self.debug, \"squashed_output\", squashed_output)\n        return squashed_output\n\n\nclass DigitCaps(nn.Module):\n    def __init__(self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16, debug=False):\n        super(DigitCaps, self).__init__()\n        self.in_channels = in_channels\n        self.num_routes = num_routes\n        self.num_capsules = num_capsules\n        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n        self.debug = debug\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n        debug_print(self.debug, \"x after stacking\", x)\n\n        W = torch.cat([self.W] * batch_size, dim=0)\n        debug_print(self.debug, \"W\", W)\n        \n        u_hat = torch.matmul(W, x)\n        debug_print(self.debug, \"u_hat\", u_hat)\n\n        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n        b_ij = b_ij.to(device)\n        debug_print(self.debug, \"b_ij\", b_ij)\n\n        num_iter = 3\n        for i in range(num_iter):\n            if self.debug: print()\n            c_ij = F.softmax(b_ij, dim=1)\n            debug_print(self.debug, \"c_ij\", c_ij)\n            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n            debug_print(self.debug, \"c_ij after repeat\", c_ij)\n\n            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n            debug_print(self.debug, \"s_j\", s_j)\n\n            v_j = squash(s_j)\n            debug_print(self.debug, \"v_j\", v_j)\n            \n            a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n            debug_print(self.debug, \"a_ij\", a_ij)\n            b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n            debug_print(self.debug, \"b_ij updated\", b_ij)\n\n        return v_j.squeeze(1)\n\nclass Decoder(nn.Module):\n    def __init__(self, debug=False):\n        super(Decoder, self).__init__()\n        self.debug = debug\n        \n    def forward(self, x, data):\n        classes = torch.sqrt((x ** 2).sum(2))\n        debug_print(self.debug, \"classes before softmax\", classes)\n        classes = F.softmax(classes, dim=0)\n        debug_print(self.debug, \"classes after softmax\", classes)\n        \n        _, max_length_indices = classes.max(dim=1)\n        debug_print(self.debug, \"max_length_indices\", max_length_indices)\n        masked = Variable(torch.sparse.torch.eye(10))\n        debug_print(self.debug, \"masked\", masked)\n        masked = masked.to(device)\n        masked = masked.index_select(dim=0, index=Variable(max_length_indices.squeeze(1).data))\n        debug_print(self.debug, \"masked after index_select\", masked)\n        \n        return masked\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CapsuleNet(nn.Module):\n    def __init__(self, debug=False):\n        super(CapsuleNet, self).__init__()\n        self.conv_layer = ConvLayer(debug=debug)\n        self.primary_capsules = PrimaryCaps(debug=debug)\n        self.digit_capsules = DigitCaps(debug=debug)\n        self.decoder = Decoder(debug=debug)\n        self.mse_loss = nn.MSELoss()\n        self.debug = debug\n        \n    def forward(self, data):\n        debug_print(self.debug, f\"Input data\", data)\n        debug_print(self.debug, \"\\nCONV\")\n        output = self.conv_layer(data)\n        debug_print(self.debug, \"\\nPRIMARY\")\n        output = self.primary_capsules(output)\n        debug_print(self.debug, \"\\nDIGIT\")\n        output = self.digit_capsules(output)\n        debug_print(self.debug, \"\\nDECODER\")\n        masked = self.decoder(output, data)\n        debug_print(self.debug, \"\\nOUTPUT\", output)\n        return output, masked\n\n    def loss(self, x, target):\n        return self.margin_loss(x, target)\n    \n    def margin_loss(self, x, labels):\n        batch_size = x.size(0)\n        v_k = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n        left = F.relu(0.9 - v_k).view(batch_size, -1)\n        right = F.relu(v_k - 0.1).view(batch_size, -1)\n        loss = labels * left + 0.5 * (1.0 - labels) * right\n        loss = loss.sum(dim=1).mean()\n        return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"capsule_net = CapsuleNet().to(device) \noptimizer = Adam(capsule_net.parameters())\n\nn_epochs = 20\n\nfor epoch in range(n_epochs):\n    capsule_net.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch_id, (data, target) in enumerate(tqdm(train_loader)):\n        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n        data, target = Variable(data), Variable(target)\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output, masked = capsule_net(data)\n        loss = capsule_net.loss(output, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n        preds = np.argmax(masked.data.cpu().numpy(), axis=1)\n        targets = np.argmax(target.data.cpu().numpy(), axis=1)\n        correct_train += np.sum(preds == targets)\n        total_train += len(targets)\n    \n    train_accuracy = correct_train / total_train\n    avg_train_loss = train_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_train_loss:.4f} - Accuracy: {train_accuracy:.4f}\")\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"capsule_net.eval()\ntest_loss = 0\ncorrect_test = 0\ntotal_test = 0\n\nwith torch.no_grad():\n    for data, target in test_loader:\n        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n        data, target = Variable(data), Variable(target)\n        data, target = data.to(device), target.to(device)\n\n        output, masked = capsule_net(data)\n        loss = capsule_net.loss(output, target)\n        test_loss += loss.item()\n\n        preds = np.argmax(masked.data.cpu().numpy(), axis=1)\n        targets = np.argmax(target.data.cpu().numpy(), axis=1)\n        correct_test += np.sum(preds == targets)\n        total_test += len(targets)\n\ntest_accuracy = correct_test / total_test\navg_test_loss = test_loss / len(test_loader)\n\nprint(f\"Test Loss: {avg_test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"capsule_net.eval()\n\nwith torch.no_grad():\n    data, target = next(iter(test_loader))\n    target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n    data, target = Variable(data), Variable(target)\n    data, target = data.to(device), target.to(device)\n    output, masked = capsule_net(data)\n    preds = np.argmax(masked.data.cpu().numpy(), axis=1)\n    targets = np.argmax(target.data.cpu().numpy(), axis=1)\n\n    fig, axes = plt.subplots(3, 5, figsize=(12, 6))\n    axes = axes.flatten()\n    \n    for i in range(15):\n        ax = axes[i]\n        ax.imshow(data[i].cpu().numpy().squeeze(), cmap='gray')\n        ax.set_title(f\"True: {targets[i]} \\nPred: {preds[i]}\")\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 256, kernel_size=9, stride=1, padding=4)\n        self.conv2 = nn.Conv2d(256, 128, kernel_size=9, stride=2, padding=4)\n        self.conv3 = nn.Conv2d(128, 64, kernel_size=9, stride=2, padding=4)\n        self.conv4 = nn.Conv2d(64, 32, kernel_size=9, stride=2, padding=4)\n        self.fc_input_size = self._get_fc_input_size()\n        self.fc = nn.Linear(self.fc_input_size, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def _get_fc_input_size(self):\n        dummy_input = torch.zeros(1, 3, 128, 128)\n        x = self.conv1(dummy_input)\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv2(x))\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv3(x))\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv4(x))\n        x = x.view(x.size(0), -1)\n        return x.size(1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv2(x))\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv3(x))\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv4(x))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = self.sigmoid(x)\n        return x\n\n    def loss(self, output, target):\n        target = target.unsqueeze(1)\n        target = target.float()\n        return F.binary_cross_entropy(output, target)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:56:36.271382Z","iopub.execute_input":"2025-03-13T20:56:36.271671Z","iopub.status.idle":"2025-03-13T20:56:36.281790Z","shell.execute_reply.started":"2025-03-13T20:56:36.271647Z","shell.execute_reply":"2025-03-13T20:56:36.280973Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"cnn = CNN().to(device)\noptimizer = Adam(cnn.parameters())\n\nn_epochs = 20\nfor epoch in range(n_epochs):\n    cnn.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch_id, (data, target) in enumerate(tqdm(dataloader)):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = cnn(data)\n        loss = cnn.loss(output, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n        preds = (output >= 0.5).float()\n        correct_train += preds.eq(target.view_as(preds)).sum().item()\n        total_train += target.size(0)\n    \n    train_accuracy = correct_train / total_train\n    avg_train_loss = train_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_train_loss:.4f} - Accuracy: {train_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:56:38.516832Z","iopub.execute_input":"2025-03-13T20:56:38.517111Z","iopub.status.idle":"2025-03-13T20:58:12.106126Z","shell.execute_reply.started":"2025-03-13T20:56:38.517089Z","shell.execute_reply":"2025-03-13T20:58:12.104931Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [00:11<00:00,  2.99s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Loss: 0.8237 - Accuracy: 0.4600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:11<00:00,  2.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Loss: 0.6798 - Accuracy: 0.6040\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:11<00:00,  2.92s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Loss: 0.6371 - Accuracy: 0.7000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:11<00:00,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Loss: 0.5743 - Accuracy: 0.6720\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:11<00:00,  2.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Loss: 0.4327 - Accuracy: 0.8240\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:11<00:00,  2.89s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 - Loss: 0.3353 - Accuracy: 0.8680\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:11<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 - Loss: 0.3127 - Accuracy: 0.8820\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 3/4 [00:11<00:03,  3.86s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-3c60fe76fdcb>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"cnn.eval()\ntest_loss = 0\ncorrect_test = 0\ntotal_test = 0\n\nfor data in test_dataloader:\n    data = data.to(device)\n    output = cnn(data)\n    target = torch.zeros(output.size(0)).float()\n    \n    target = target.to(device)\n    loss = cnn.loss(output, target)\n\n    test_loss += loss.item()\n\n    preds = (output >= 0.5).float()\n    correct_test += preds.eq(target.view_as(preds)).sum().item()\n    total_test += target.size(0)\n\ntest_accuracy = correct_test / total_test\navg_test_loss = test_loss / len(test_dataloader)\nprint(f\"Loss: {avg_test_loss:.4f} - Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T21:00:40.744475Z","iopub.execute_input":"2025-03-13T21:00:40.744808Z","iopub.status.idle":"2025-03-13T21:00:43.140077Z","shell.execute_reply.started":"2025-03-13T21:00:40.744779Z","shell.execute_reply":"2025-03-13T21:00:43.139329Z"}},"outputs":[{"name":"stdout","text":"Loss: 1.9946 - Accuracy: 0.0859\n","output_type":"stream"}],"execution_count":19}]}