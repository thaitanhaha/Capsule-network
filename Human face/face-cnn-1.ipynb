{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1504266,"sourceType":"datasetVersion","datasetId":885385},{"sourceId":1960165,"sourceType":"datasetVersion","datasetId":1170109},{"sourceId":227433307,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Subset\nfrom torch.utils.data import DataLoader\nfrom collections import Counter\nfrom torch.optim import Adam\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:13:17.235093Z","iopub.execute_input":"2025-03-14T10:13:17.235462Z","iopub.status.idle":"2025-03-14T10:13:24.333700Z","shell.execute_reply.started":"2025-03-14T10:13:17.235433Z","shell.execute_reply":"2025-03-14T10:13:24.333011Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"human_faces_dir = '/kaggle/input/human-faces/Humans'\nflowers_dir = '/kaggle/input/face-dataset/human-swap/'\n\nhuman_faces_files = len(os.listdir(human_faces_dir))\nflowers_files = len(os.listdir(flowers_dir))\n\nprint(f\"Number of files in human_faces_dir: {human_faces_files}\")\nprint(f\"Number of files in flowers_dir: {flowers_files}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:22:35.560177Z","iopub.execute_input":"2025-03-14T10:22:35.560517Z","iopub.status.idle":"2025-03-14T10:22:35.574389Z","shell.execute_reply.started":"2025-03-14T10:22:35.560488Z","shell.execute_reply":"2025-03-14T10:22:35.573511Z"}},"outputs":[{"name":"stdout","text":"Number of files in human_faces_dir: 7219\nNumber of files in flowers_dir: 6676\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"my_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\nclass CustomDataset(Dataset):\n    def __init__(self, human_faces_dir, flowers_dir, transform=None):\n        self.human_faces_dir = human_faces_dir\n        self.flowers_dir = flowers_dir\n        \n        human_faces_images = [os.path.join(human_faces_dir, fname) for fname in os.listdir(human_faces_dir)]\n        self.human_faces_images = random.sample(human_faces_images, 300)\n        flowers_images = [os.path.join(flowers_dir, fname) for fname in os.listdir(flowers_dir)]\n        self.flowers_images = random.sample(flowers_images, 200)\n        \n        self.all_images = self.human_faces_images + self.flowers_images\n        self.labels = [1] * len(self.human_faces_images) + [0] * len(self.flowers_images)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.all_images)\n\n    def __getitem__(self, idx):\n        img_path = self.all_images[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        label = self.labels[idx]\n        \n        if self.transform: image = self.transform(image)\n        return image, label\n\ndataset = CustomDataset(human_faces_dir, flowers_dir, transform=my_transform)\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:22:40.091763Z","iopub.execute_input":"2025-03-14T10:22:40.092123Z","iopub.status.idle":"2025-03-14T10:22:40.117745Z","shell.execute_reply.started":"2025-03-14T10:22:40.092083Z","shell.execute_reply":"2025-03-14T10:22:40.116864Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"for images, labels in dataloader:\n    print(images.shape)\n    print(labels)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:22:42.619332Z","iopub.execute_input":"2025-03-14T10:22:42.619602Z","iopub.status.idle":"2025-03-14T10:22:47.183419Z","shell.execute_reply.started":"2025-03-14T10:22:42.619580Z","shell.execute_reply":"2025-03-14T10:22:47.182429Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 128, 128])\ntensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n        1, 0, 1, 0, 0, 0, 1, 1])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, test_dir, transform=None):\n        self.test_dir = test_dir\n        test_images = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith(('.jpg', '.png', '.jpeg'))]\n        test_images = random.sample(test_images, 128)\n        self.all_images = test_images\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.all_images)\n\n    def __getitem__(self, idx):\n        img_path = self.all_images[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform: image = self.transform(image)\n        return image\n\ntest_dir = \"/kaggle/input/face-dataset/human-swap/\"\ntest_dataset = TestDataset(test_dir, transform=my_transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:22:47.184572Z","iopub.execute_input":"2025-03-14T10:22:47.184824Z","iopub.status.idle":"2025-03-14T10:22:47.200720Z","shell.execute_reply.started":"2025-03-14T10:22:47.184800Z","shell.execute_reply":"2025-03-14T10:22:47.199785Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"for images in test_dataloader:\n    print(images.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:22:47.202561Z","iopub.execute_input":"2025-03-14T10:22:47.202876Z","iopub.status.idle":"2025-03-14T10:22:50.152229Z","shell.execute_reply.started":"2025-03-14T10:22:47.202851Z","shell.execute_reply":"2025-03-14T10:22:50.151350Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 128, 128])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## CNN","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 256, kernel_size=9, stride=1, padding=4)\n        self.conv2 = nn.Conv2d(256, 128, kernel_size=9, stride=2, padding=4)\n        self.conv3 = nn.Conv2d(128, 64, kernel_size=9, stride=2, padding=4)\n        self.conv4 = nn.Conv2d(64, 32, kernel_size=9, stride=2, padding=4)\n        self.fc_input_size = self._get_fc_input_size()\n        self.fc = nn.Linear(self.fc_input_size, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def _get_fc_input_size(self):\n        dummy_input = torch.zeros(1, 3, 128, 128)\n        x = self.conv1(dummy_input)\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv2(x))\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv3(x))\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv4(x))\n        x = x.view(x.size(0), -1)\n        return x.size(1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv2(x))\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv3(x))\n        x = F.relu(F.max_pool2d(x, 2))\n        x = F.relu(self.conv4(x))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = self.sigmoid(x)\n        return x\n\n    def loss(self, output, target):\n        target = target.unsqueeze(1)\n        target = target.float()\n        return F.binary_cross_entropy(output, target)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:22:52.931307Z","iopub.execute_input":"2025-03-14T10:22:52.931597Z","iopub.status.idle":"2025-03-14T10:22:52.940911Z","shell.execute_reply.started":"2025-03-14T10:22:52.931575Z","shell.execute_reply":"2025-03-14T10:22:52.939916Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"cnn = CNN().to(device)\noptimizer = Adam(cnn.parameters())\n\nn_epochs = 20\nfor epoch in range(n_epochs):\n    cnn.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch_id, (data, target) in enumerate(tqdm(dataloader)):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = cnn(data)\n        loss = cnn.loss(output, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n        preds = (output >= 0.5).float()\n        correct_train += preds.eq(target.view_as(preds)).sum().item()\n        total_train += target.size(0)\n    \n    train_accuracy = correct_train / total_train\n    avg_train_loss = train_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_train_loss:.4f} - Accuracy: {train_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:22:54.667829Z","iopub.execute_input":"2025-03-14T10:22:54.668243Z","iopub.status.idle":"2025-03-14T10:27:05.973437Z","shell.execute_reply.started":"2025-03-14T10:22:54.668211Z","shell.execute_reply":"2025-03-14T10:27:05.972557Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [00:16<00:00,  4.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Loss: 1.3310 - Accuracy: 0.5200\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Loss: 0.6982 - Accuracy: 0.4000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Loss: 0.6975 - Accuracy: 0.4000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Loss: 0.6967 - Accuracy: 0.4000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Loss: 0.6951 - Accuracy: 0.4000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 - Loss: 0.6931 - Accuracy: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 - Loss: 0.6903 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 - Loss: 0.6871 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 - Loss: 0.6824 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 - Loss: 0.6783 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 - Loss: 0.6740 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 - Loss: 0.6739 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 - Loss: 0.6760 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 - Loss: 0.6739 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 - Loss: 0.6740 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 - Loss: 0.6731 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 - Loss: 0.6730 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 - Loss: 0.6731 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 - Loss: 0.6728 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:12<00:00,  3.05s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 - Loss: 0.6730 - Accuracy: 0.6000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"cnn.eval()\ntest_loss = 0\ncorrect_test = 0\ntotal_test = 0\n\nfor data in test_dataloader:\n    data = data.to(device)\n    output = cnn(data)\n    target = torch.zeros(output.size(0)).float()\n    \n    target = target.to(device)\n    loss = cnn.loss(output, target)\n\n    test_loss += loss.item()\n\n    preds = (output >= 0.5).float()\n    correct_test += preds.eq(target.view_as(preds)).sum().item()\n    total_test += target.size(0)\n\ntest_accuracy = correct_test / total_test\navg_test_loss = test_loss / len(test_dataloader)\nprint(f\"Loss: {avg_test_loss:.4f} - Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:27:21.812159Z","iopub.execute_input":"2025-03-14T10:27:21.812588Z","iopub.status.idle":"2025-03-14T10:27:23.649488Z","shell.execute_reply.started":"2025-03-14T10:27:21.812548Z","shell.execute_reply":"2025-03-14T10:27:23.648774Z"}},"outputs":[{"name":"stdout","text":"Loss: 0.9030 - Accuracy: 0.0000\n","output_type":"stream"}],"execution_count":17}]}